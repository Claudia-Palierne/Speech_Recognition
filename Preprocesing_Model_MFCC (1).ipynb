{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "t1MPivN4HEjJ",
        "outputId": "bd4b6135-ad29-46d1-e66d-5b80f8287de3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: tensorflow_io in /usr/local/lib/python3.10/dist-packages (0.32.0)\n",
            "Requirement already satisfied: tensorflow-io-gcs-filesystem==0.32.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow_io) (0.32.0)\n"
          ]
        }
      ],
      "source": [
        "!pip install tensorflow_io"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CAj9-c1l00Q9",
        "outputId": "0da22b9c-af79-43f5-cdf3-9af364da1418"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "Wqur42BLZEq3"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import wave\n",
        "import librosa\n",
        "import random\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from glob import glob\n",
        "import IPython.display as ipd\n",
        "import tensorflow as tf\n",
        "import tensorflow_io as tfio\n",
        "from functools import reduce"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "49nqFdV8Af7g"
      },
      "source": [
        "# Speech Recognition"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "hAIDWjjAAnKs"
      },
      "outputs": [],
      "source": [
        "path = '/content/drive/MyDrive/Kaggle/speech_recognition/'\n",
        "\n",
        "#path = '/content/gdrive/MyDrive/speech_recognition/'\n",
        "classes = ['yes', 'no', 'one', 'two', 'three', 'four', 'five', 'six', 'seven', 'eight', 'nine', 'up', 'down']"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vgmJV9NX-jIa"
      },
      "source": [
        "# Building dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "-rCkMSM1lGMD"
      },
      "outputs": [],
      "source": [
        "# Define Paths to labeled data and Split into Train, Validation and Test using Kaggle split\n",
        "tf_dic_train = {}\n",
        "tf_dic_val = {}\n",
        "tf_dic_test = {}\n",
        "FILES_TO_LOAD = 500\n",
        "\n",
        "with open(os.path.join(path + \"/testing_list.txt\"), 'r') as file:\n",
        "    test_names = file.read().splitlines()\n",
        "with open(os.path.join(path + \"/validation_list.txt\"), 'r') as file:\n",
        "    val_names = file.read().splitlines()\n",
        "\n",
        "for i, label in enumerate(classes):\n",
        "  train_tmp = []\n",
        "  val_tmp = []\n",
        "  test_tmp = []\n",
        "  for filename in os.listdir(os.path.join(path + label)):\n",
        "    if \"/\".join([label, filename]) in test_names:\n",
        "      if len(test_tmp) >= FILES_TO_LOAD*0.1:\n",
        "        pass\n",
        "      else:\n",
        "        test_tmp.append(os.path.join(path + label + \"/\" + filename))\n",
        "    elif \"/\".join([label, filename]) in val_names:\n",
        "      if len(val_tmp) >= FILES_TO_LOAD*0.1:\n",
        "        pass\n",
        "      else:\n",
        "        val_tmp.append(os.path.join(path + label + \"/\" + filename))\n",
        "    else:\n",
        "      if len(train_tmp) >= FILES_TO_LOAD*0.8:\n",
        "        pass\n",
        "      else:\n",
        "        train_tmp.append(os.path.join(path + label + \"/\" + filename))\n",
        "\n",
        "  train_tmp_tf = tf.data.Dataset.list_files(train_tmp)\n",
        "  val_tmp_tf = tf.data.Dataset.list_files(val_tmp)\n",
        "  test_tmp_tf = tf.data.Dataset.list_files(test_tmp)\n",
        "\n",
        "  tf_dic_train[label] = tf.data.Dataset.zip((train_tmp_tf, tf.data.Dataset.from_tensor_slices(tf.fill((len(train_tmp_tf),), i))))\n",
        "  tf_dic_val[label] = tf.data.Dataset.zip((val_tmp_tf, tf.data.Dataset.from_tensor_slices(tf.fill((len(val_tmp_tf),), i))))\n",
        "  tf_dic_test[label] = tf.data.Dataset.zip((test_tmp_tf, tf.data.Dataset.from_tensor_slices(tf.fill((len(test_tmp_tf),), i))))\n",
        "\n",
        "datasets_train = list(tf_dic_train.values())\n",
        "datasets_val = list(tf_dic_val.values())\n",
        "datasets_test = list(tf_dic_test.values())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "CeOND8VnxWAi"
      },
      "outputs": [],
      "source": [
        "def dataset_reduce(datasets):\n",
        "  merged_dataset_reduce = reduce(lambda d1, d2: d1.concatenate(d2), datasets)\n",
        "  return merged_dataset_reduce.shuffle(buffer_size=1000)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "qp88sR5exlkC"
      },
      "outputs": [],
      "source": [
        "datasets_train = dataset_reduce(datasets_train)\n",
        "datasets_val = dataset_reduce(datasets_val)\n",
        "datasets_test = dataset_reduce(datasets_test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ApnE-Lh_yCc-",
        "outputId": "fee7188e-4d01-4671-aa37-7cb6987be58d"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(3120, 466, 486)"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ],
      "source": [
        "len(datasets_train), len(datasets_val), len(datasets_test)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_gNU6oDy-dRt"
      },
      "source": [
        "# Preprocessing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "fDwJKqP-veEG"
      },
      "outputs": [],
      "source": [
        "def load_wav_16k_mono(filename):\n",
        "    # Load encoded wav file\n",
        "    file_contents = tf.io.read_file(filename)\n",
        "    # Decode wav (tensors by channels)\n",
        "    wav, sample_rate = tf.audio.decode_wav(file_contents, desired_channels=1)\n",
        "    # Removes trailing axis\n",
        "    wav = tf.squeeze(wav, axis=-1)\n",
        "    sample_rate = tf.cast(sample_rate, dtype=tf.int64)\n",
        "    wav = tfio.audio.resample(wav, rate_in=sample_rate, rate_out=16000)\n",
        "    return wav"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "J3nFfroB3j1a"
      },
      "outputs": [],
      "source": [
        "# Load padding wav file from _noise folder (/content/gdrive/MyDrive/speech_recognition/_noise/dude_miaowing.wav)\n",
        "padding_file = '/content/drive/MyDrive/Kaggle/speech_recognition/_noise/dude_miaowing.wav'\n",
        "padding_contents = tf.io.read_file(padding_file)\n",
        "padding_waveform, _ = tf.audio.decode_wav(padding_contents, desired_channels=1)\n",
        "padding_waveform = tf.squeeze(padding_waveform, axis=-1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "48W8Ev9yPOns"
      },
      "outputs": [],
      "source": [
        "def preprocess_multy(file_path, label, style=\"mfcc\"):\n",
        "    wav = load_wav_16k_mono(file_path)\n",
        "    wav = wav[:16000]\n",
        "    # print(type(wav))\n",
        "\n",
        "    # Calculate difference in length\n",
        "    target_length = 16000\n",
        "    current_length = tf.shape(wav)[0]\n",
        "    pad_length = target_length - current_length\n",
        "\n",
        "    # Pad waveform if it is shorter than the target length\n",
        "    if pad_length > 0:\n",
        "        padding_start = tf.random.uniform(shape=[], minval=0, maxval=padding_waveform.shape[0] - pad_length, dtype=tf.int32)\n",
        "        padding_slice = padding_waveform[padding_start : padding_start + pad_length]\n",
        "\n",
        "        # Pad the waveform with the selected slice\n",
        "        wav = tf.concat([wav, padding_slice], axis=0)\n",
        "\n",
        "    # print(type(wav))\n",
        "\n",
        "    # Build the output, i.e. spectrogram or mfcc\n",
        "\n",
        "    spc = tf.signal.stft(wav, frame_length=320, frame_step=32)\n",
        "    spc = tf.abs(spc)\n",
        "\n",
        "    # Define shape for the spectrogram tensor\n",
        "    spectrogram_shape = tf.shape(spc)\n",
        "\n",
        "    if style==\"mfcc\":\n",
        "      # Compute MFCCs\n",
        "      mel_filterbank = tf.signal.linear_to_mel_weight_matrix(\n",
        "      num_mel_bins=40,\n",
        "      num_spectrogram_bins=spectrogram_shape[-1],\n",
        "      sample_rate=target_length\n",
        "      )\n",
        "      mel_spectrogram = tf.matmul(spc, mel_filterbank)\n",
        "      log_mel_spectrogram = tf.math.log(mel_spectrogram + 1e-6)\n",
        "      mfccs = tf.signal.mfccs_from_log_mel_spectrograms(log_mel_spectrogram)[:, :20]  # Keep only the first 20 coefficients\n",
        "      img = tf.expand_dims(mfccs, axis=2)\n",
        "    else:\n",
        "      img = tf.expand_dims(spc, axis=2)\n",
        "\n",
        "    return img, tf.one_hot(label, 13)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "m1DC4ZrfPP0_",
        "outputId": "68e7091f-ad10-4aed-beec-2b4bb08cdbc4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:Using a while_loop for converting IO>AudioResample cause there is no registered converter for this op.\n"
          ]
        }
      ],
      "source": [
        "# Train data:\n",
        "train_data = datasets_train.map(preprocess_multy)\n",
        "train_data = train_data.cache()\n",
        "train_data = train_data.shuffle(buffer_size=1000)\n",
        "train_data = train_data.batch(16)\n",
        "train_data = train_data.prefetch(8)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5E5nWR3N5M-W",
        "outputId": "651d80e9-233d-48ab-c0e0-e68603a81b63"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:Using a while_loop for converting IO>AudioResample cause there is no registered converter for this op.\n"
          ]
        }
      ],
      "source": [
        "# Validation data:\n",
        "val_data = datasets_val.map(preprocess_multy)\n",
        "val_data = val_data.cache()\n",
        "val_data = val_data.shuffle(buffer_size=1000)\n",
        "val_data = val_data.batch(16)\n",
        "val_data = val_data.prefetch(8)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nilWusoI5ZUV",
        "outputId": "1a0b1d45-eeed-4a26-c436-93af5da8bc11"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:Using a while_loop for converting IO>AudioResample cause there is no registered converter for this op.\n"
          ]
        }
      ],
      "source": [
        "# Test data:\n",
        "test_data = datasets_test.map(preprocess_multy)\n",
        "test_data = test_data.cache()\n",
        "test_data = test_data.shuffle(buffer_size=1000)\n",
        "test_data = test_data.batch(16)\n",
        "test_data = test_data.prefetch(8)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9P_zV44143yj",
        "outputId": "b5a18306-1974-49e3-cccc-56963556416a"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(195, 30, 31)"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ],
      "source": [
        "len(train_data), len(val_data), len(test_data)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "cA9AQKXIRbCZ"
      },
      "outputs": [],
      "source": [
        "# samples, labels = train_data.as_numpy_iterator().next()\n",
        "# samples.shape, labels.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "6gG8eE_67-C1"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3k93Fn-QFcu_"
      },
      "source": [
        "# Base line model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "dkr_riAlFhNA"
      },
      "outputs": [],
      "source": [
        "# Build the baseline model\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Conv2D, Dense, Flatten\n",
        "\n",
        "base_model = Sequential()\n",
        "base_model.add(Conv2D(16, (3,3), activation='relu', input_shape=(491, 20, 1)))\n",
        "base_model.add(Flatten())\n",
        "base_model.add(Dense(len(classes), activation='softmax'))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gL2isvpVFyYz",
        "outputId": "92ddb92a-a4f9-4e20-e362-e5e2d8925fd0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " conv2d (Conv2D)             (None, 489, 18, 16)       160       \n",
            "                                                                 \n",
            " flatten (Flatten)           (None, 140832)            0         \n",
            "                                                                 \n",
            " dense (Dense)               (None, 13)                1830829   \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 1,830,989\n",
            "Trainable params: 1,830,989\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ]
        }
      ],
      "source": [
        "base_model.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "elXFxeSeF06Q"
      },
      "outputs": [],
      "source": [
        "base_model.compile('Adam', loss='CategoricalCrossentropy', metrics=['accuracy', tf.keras.metrics.Recall(),tf.keras.metrics.Precision()])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gIU5hA7_F3ig",
        "outputId": "aaad90ca-cdd6-43f4-da60-458759b8e48e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n",
            "195/195 [==============================] - 1096s 4s/step - loss: 4.9248 - accuracy: 0.4051 - recall: 0.3022 - precision: 0.4404 - val_loss: 1.6547 - val_accuracy: 0.4700 - val_recall: 0.2489 - val_precision: 0.6374\n",
            "Epoch 2/10\n",
            "195/195 [==============================] - 1s 5ms/step - loss: 0.8401 - accuracy: 0.7503 - recall: 0.6324 - precision: 0.8519 - val_loss: 1.6813 - val_accuracy: 0.4979 - val_recall: 0.3798 - val_precision: 0.6189\n",
            "Epoch 3/10\n",
            "195/195 [==============================] - 1s 6ms/step - loss: 0.3313 - accuracy: 0.9183 - recall: 0.8760 - precision: 0.9503 - val_loss: 1.8762 - val_accuracy: 0.5043 - val_recall: 0.4013 - val_precision: 0.5500\n",
            "Epoch 4/10\n",
            "195/195 [==============================] - 1s 7ms/step - loss: 0.1338 - accuracy: 0.9702 - recall: 0.9574 - precision: 0.9787 - val_loss: 1.9922 - val_accuracy: 0.5193 - val_recall: 0.4442 - val_precision: 0.5640\n",
            "Epoch 5/10\n",
            "195/195 [==============================] - 1s 5ms/step - loss: 0.0701 - accuracy: 0.9856 - recall: 0.9798 - precision: 0.9896 - val_loss: 2.0019 - val_accuracy: 0.5365 - val_recall: 0.4936 - val_precision: 0.5823\n",
            "Epoch 6/10\n",
            "195/195 [==============================] - 1s 5ms/step - loss: 0.0227 - accuracy: 0.9962 - recall: 0.9955 - precision: 0.9962 - val_loss: 2.3103 - val_accuracy: 0.5021 - val_recall: 0.4700 - val_precision: 0.5341\n",
            "Epoch 7/10\n",
            "195/195 [==============================] - 1s 5ms/step - loss: 0.0373 - accuracy: 0.9926 - recall: 0.9904 - precision: 0.9932 - val_loss: 2.3078 - val_accuracy: 0.5172 - val_recall: 0.4893 - val_precision: 0.5575\n",
            "Epoch 8/10\n",
            "195/195 [==============================] - 1s 5ms/step - loss: 0.0063 - accuracy: 0.9997 - recall: 0.9997 - precision: 1.0000 - val_loss: 2.3298 - val_accuracy: 0.5408 - val_recall: 0.5107 - val_precision: 0.5680\n",
            "Epoch 9/10\n",
            "195/195 [==============================] - 1s 5ms/step - loss: 0.0029 - accuracy: 1.0000 - recall: 1.0000 - precision: 1.0000 - val_loss: 2.3855 - val_accuracy: 0.5408 - val_recall: 0.5129 - val_precision: 0.5664\n",
            "Epoch 10/10\n",
            "195/195 [==============================] - 1s 5ms/step - loss: 0.0018 - accuracy: 1.0000 - recall: 1.0000 - precision: 1.0000 - val_loss: 2.4460 - val_accuracy: 0.5515 - val_recall: 0.5322 - val_precision: 0.5767\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7f4d5a510070>"
            ]
          },
          "metadata": {},
          "execution_count": 20
        }
      ],
      "source": [
        "base_model.fit(train_data, validation_data=val_data, epochs=10)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "17QNY5zYGTVf"
      },
      "source": [
        "# AlexNet inspiration Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eA1jQk7v7c1G",
        "outputId": "64be2c21-c123-438a-97b0-d69271086ca1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential_1\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " conv2d_1 (Conv2D)           (None, 489, 18, 64)       640       \n",
            "                                                                 \n",
            " dropout (Dropout)           (None, 489, 18, 64)       0         \n",
            "                                                                 \n",
            " conv2d_2 (Conv2D)           (None, 487, 16, 128)      73856     \n",
            "                                                                 \n",
            " max_pooling2d (MaxPooling2D  (None, 243, 8, 128)      0         \n",
            " )                                                               \n",
            "                                                                 \n",
            " dropout_1 (Dropout)         (None, 243, 8, 128)       0         \n",
            "                                                                 \n",
            " conv2d_3 (Conv2D)           (None, 241, 6, 256)       295168    \n",
            "                                                                 \n",
            " max_pooling2d_1 (MaxPooling  (None, 120, 3, 256)      0         \n",
            " 2D)                                                             \n",
            "                                                                 \n",
            " dropout_2 (Dropout)         (None, 120, 3, 256)       0         \n",
            "                                                                 \n",
            " flatten_1 (Flatten)         (None, 92160)             0         \n",
            "                                                                 \n",
            " dense_1 (Dense)             (None, 128)               11796608  \n",
            "                                                                 \n",
            " dropout_3 (Dropout)         (None, 128)               0         \n",
            "                                                                 \n",
            " dense_2 (Dense)             (None, 13)                1677      \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 12,167,949\n",
            "Trainable params: 12,167,949\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ]
        }
      ],
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "\n",
        "# Define the CNN model\n",
        "model = tf.keras.Sequential([\n",
        "    layers.Conv2D(64, (3, 3), activation='relu', input_shape=(491, 20, 1)),\n",
        "    layers.Dropout(0.5),\n",
        "    layers.Conv2D(128, (3, 3), activation='relu'),\n",
        "    layers.MaxPooling2D((2, 2)),\n",
        "    layers.Dropout(0.5),\n",
        "    layers.Conv2D(256, (3, 3), activation='relu'),\n",
        "    layers.MaxPooling2D((2, 2)),\n",
        "    layers.Dropout(0.5),\n",
        "    layers.Flatten(),\n",
        "    layers.Dense(128, activation='relu'),\n",
        "    layers.Dropout(0.5),\n",
        "    layers.Dense(13, activation='softmax')\n",
        "])\n",
        "\n",
        "# Compile the model\n",
        "model.compile(optimizer=keras.optimizers.Adam(learning_rate=0.001),\n",
        "              loss=tf.keras.losses.CategoricalCrossentropy(),\n",
        "              metrics=['accuracy', tf.keras.metrics.Recall(),tf.keras.metrics.Precision()])\n",
        "\n",
        "# Print model summary\n",
        "model.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "F3DW73Lr8AQP",
        "outputId": "2e1d33d1-7bc4-478c-dc54-5dbd733f98f1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/25\n",
            "195/195 [==============================] - 12s 41ms/step - loss: 2.6658 - accuracy: 0.1436 - recall_1: 0.0051 - precision_1: 0.2254 - val_loss: 2.4972 - val_accuracy: 0.0987 - val_recall_1: 0.0000e+00 - val_precision_1: 0.0000e+00\n",
            "Epoch 2/25\n",
            "195/195 [==============================] - 8s 40ms/step - loss: 2.3456 - accuracy: 0.1942 - recall_1: 0.0115 - precision_1: 0.3103 - val_loss: 2.1304 - val_accuracy: 0.3498 - val_recall_1: 0.0129 - val_precision_1: 0.7500\n",
            "Epoch 3/25\n",
            "195/195 [==============================] - 8s 39ms/step - loss: 1.9909 - accuracy: 0.3340 - recall_1: 0.0840 - precision_1: 0.5293 - val_loss: 1.7584 - val_accuracy: 0.4356 - val_recall_1: 0.1094 - val_precision_1: 0.8500\n",
            "Epoch 4/25\n",
            "195/195 [==============================] - 8s 40ms/step - loss: 1.7480 - accuracy: 0.4128 - recall_1: 0.1776 - precision_1: 0.6375 - val_loss: 1.5265 - val_accuracy: 0.5193 - val_recall_1: 0.1888 - val_precision_1: 0.8148\n",
            "Epoch 5/25\n",
            "195/195 [==============================] - 7s 37ms/step - loss: 1.5462 - accuracy: 0.4904 - recall_1: 0.2657 - precision_1: 0.6801 - val_loss: 1.5712 - val_accuracy: 0.5193 - val_recall_1: 0.1395 - val_precision_1: 0.8333\n",
            "Epoch 6/25\n",
            "195/195 [==============================] - 8s 39ms/step - loss: 1.4616 - accuracy: 0.5279 - recall_1: 0.3186 - precision_1: 0.7156 - val_loss: 1.3739 - val_accuracy: 0.5322 - val_recall_1: 0.2876 - val_precision_1: 0.8024\n",
            "Epoch 7/25\n",
            "195/195 [==============================] - 8s 40ms/step - loss: 1.3202 - accuracy: 0.5721 - recall_1: 0.3843 - precision_1: 0.7452 - val_loss: 1.2798 - val_accuracy: 0.5815 - val_recall_1: 0.3219 - val_precision_1: 0.8065\n",
            "Epoch 8/25\n",
            "195/195 [==============================] - 8s 38ms/step - loss: 1.2166 - accuracy: 0.5939 - recall_1: 0.4359 - precision_1: 0.7493 - val_loss: 1.1888 - val_accuracy: 0.6180 - val_recall_1: 0.4163 - val_precision_1: 0.7823\n",
            "Epoch 9/25\n",
            "195/195 [==============================] - 7s 37ms/step - loss: 1.1297 - accuracy: 0.6282 - recall_1: 0.4753 - precision_1: 0.7555 - val_loss: 1.1695 - val_accuracy: 0.6073 - val_recall_1: 0.4077 - val_precision_1: 0.7631\n",
            "Epoch 10/25\n",
            "195/195 [==============================] - 8s 39ms/step - loss: 1.0063 - accuracy: 0.6622 - recall_1: 0.5462 - precision_1: 0.7922 - val_loss: 1.1442 - val_accuracy: 0.6330 - val_recall_1: 0.4614 - val_precision_1: 0.8022\n",
            "Epoch 11/25\n",
            "195/195 [==============================] - 8s 39ms/step - loss: 0.9510 - accuracy: 0.6795 - recall_1: 0.5654 - precision_1: 0.8036 - val_loss: 1.1470 - val_accuracy: 0.6717 - val_recall_1: 0.5408 - val_precision_1: 0.8235\n",
            "Epoch 12/25\n",
            "195/195 [==============================] - 7s 37ms/step - loss: 0.9039 - accuracy: 0.6920 - recall_1: 0.5987 - precision_1: 0.8021 - val_loss: 1.1159 - val_accuracy: 0.6631 - val_recall_1: 0.4764 - val_precision_1: 0.8222\n",
            "Epoch 13/25\n",
            "195/195 [==============================] - 7s 37ms/step - loss: 0.8151 - accuracy: 0.7240 - recall_1: 0.6397 - precision_1: 0.8190 - val_loss: 1.0893 - val_accuracy: 0.6609 - val_recall_1: 0.5258 - val_precision_1: 0.8007\n",
            "Epoch 14/25\n",
            "195/195 [==============================] - 8s 39ms/step - loss: 0.7794 - accuracy: 0.7295 - recall_1: 0.6500 - precision_1: 0.8201 - val_loss: 1.0672 - val_accuracy: 0.6910 - val_recall_1: 0.5601 - val_precision_1: 0.8080\n",
            "Epoch 15/25\n",
            "195/195 [==============================] - 7s 37ms/step - loss: 0.7230 - accuracy: 0.7622 - recall_1: 0.6862 - precision_1: 0.8439 - val_loss: 1.1921 - val_accuracy: 0.6588 - val_recall_1: 0.5515 - val_precision_1: 0.7764\n",
            "Epoch 16/25\n",
            "195/195 [==============================] - 7s 38ms/step - loss: 0.6693 - accuracy: 0.7728 - recall_1: 0.7003 - precision_1: 0.8378 - val_loss: 1.0562 - val_accuracy: 0.6781 - val_recall_1: 0.5944 - val_precision_1: 0.7673\n",
            "Epoch 17/25\n",
            "195/195 [==============================] - 8s 39ms/step - loss: 0.6453 - accuracy: 0.7753 - recall_1: 0.7119 - precision_1: 0.8400 - val_loss: 1.0206 - val_accuracy: 0.7146 - val_recall_1: 0.6030 - val_precision_1: 0.8240\n",
            "Epoch 18/25\n",
            "195/195 [==============================] - 7s 38ms/step - loss: 0.6301 - accuracy: 0.7897 - recall_1: 0.7301 - precision_1: 0.8456 - val_loss: 1.1054 - val_accuracy: 0.6803 - val_recall_1: 0.5751 - val_precision_1: 0.7768\n",
            "Epoch 19/25\n",
            "195/195 [==============================] - 7s 38ms/step - loss: 0.5995 - accuracy: 0.7904 - recall_1: 0.7365 - precision_1: 0.8568 - val_loss: 1.0263 - val_accuracy: 0.6867 - val_recall_1: 0.6288 - val_precision_1: 0.7984\n",
            "Epoch 20/25\n",
            "195/195 [==============================] - 7s 37ms/step - loss: 0.5827 - accuracy: 0.8003 - recall_1: 0.7513 - precision_1: 0.8558 - val_loss: 1.1112 - val_accuracy: 0.6910 - val_recall_1: 0.6288 - val_precision_1: 0.7793\n"
          ]
        }
      ],
      "source": [
        "early_stopping = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=3)\n",
        "model_checkpoint = tf.keras.callbacks.ModelCheckpoint('model.h5', monitor='val_accuracy', save_best_only=True)\n",
        "\n",
        "# Train the model with callbacks\n",
        "history = model.fit(train_data,\n",
        "                    epochs=25,\n",
        "                    validation_data=val_data,\n",
        "                    callbacks=[early_stopping, model_checkpoint])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "id": "5SXgLYFS4TOq",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7383b6d3-cdb4-49b0-d871-4697a49cd641"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "31/31 [==============================] - 0s 11ms/step - loss: 1.6392 - accuracy: 0.6152 - recall_1: 0.5206 - precision_1: 0.7047\n",
            "Test Loss: 1.639186978340149\n",
            "Test Accuracy: 0.6152263283729553\n",
            "Test Recall: 0.5205761194229126\n",
            "Test Precision: 0.7047353982925415\n"
          ]
        }
      ],
      "source": [
        "# Evaluate the model on the test data\n",
        "test_loss, test_accuracy, test_recall, test_precision = model.evaluate(test_data)\n",
        "\n",
        "print('Test Loss:', test_loss)\n",
        "print('Test Accuracy:', test_accuracy)\n",
        "print('Test Recall:', test_recall)\n",
        "print('Test Precision:', test_precision)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ResNET (Future work)"
      ],
      "metadata": {
        "id": "VA_j7mlnpJkY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras import layers\n",
        "from tensorflow.keras.applications import ResNet50\n",
        "\n",
        "# Load pre-trained ResNet50 model without the top classification layer\n",
        "resnet = ResNet50(weights='imagenet', include_top=False, input_shape=(32, 32, 1))\n",
        "\n",
        "# Freeze the layers in the base model\n",
        "resnet.trainable = False\n",
        "\n",
        "# Create a new model by adding a global average pooling layer and a dense output layer\n",
        "resnet_model = tf.keras.Sequential([\n",
        "    tf.keras.layers.Resizing(32,32),\n",
        "    resnet,\n",
        "    layers.GlobalAveragePooling2D(),\n",
        "    layers.Dense(13, activation='softmax')\n",
        "])\n",
        "\n",
        "# Compile the model\n",
        "resnet_model.compile(optimizer='adam',\n",
        "              loss=tf.keras.losses.CategoricalCrossentropy(),\n",
        "              metrics=['accuracy', tf.keras.metrics.Recall(), tf.keras.metrics.Precision()])\n",
        "\n",
        "# Print model summary\n",
        "resnet_model.summary()"
      ],
      "metadata": {
        "id": "KJxWur7qpLQP",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 398
        },
        "outputId": "417e120e-83d0-4c8c-86a6-459a816cdff5"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-29-9696c0936723>\u001b[0m in \u001b[0;36m<cell line: 6>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;31m# Load pre-trained ResNet50 model without the top classification layer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0mresnet\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mResNet50\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mweights\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'imagenet'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minclude_top\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_shape\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m32\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m32\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;31m# Freeze the layers in the base model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/keras/applications/resnet.py\u001b[0m in \u001b[0;36mResNet50\u001b[0;34m(include_top, weights, input_tensor, input_shape, pooling, classes, **kwargs)\u001b[0m\n\u001b[1;32m    519\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mstack1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m512\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"conv5\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    520\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 521\u001b[0;31m     return ResNet(\n\u001b[0m\u001b[1;32m    522\u001b[0m         \u001b[0mstack_fn\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    523\u001b[0m         \u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/keras/applications/resnet.py\u001b[0m in \u001b[0;36mResNet\u001b[0;34m(stack_fn, preact, use_bias, model_name, include_top, weights, input_tensor, input_shape, pooling, classes, classifier_activation, **kwargs)\u001b[0m\n\u001b[1;32m    157\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    158\u001b[0m     \u001b[0;31m# Determine proper input shape\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 159\u001b[0;31m     input_shape = imagenet_utils.obtain_input_shape(\n\u001b[0m\u001b[1;32m    160\u001b[0m         \u001b[0minput_shape\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    161\u001b[0m         \u001b[0mdefault_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m224\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/keras/applications/imagenet_utils.py\u001b[0m in \u001b[0;36mobtain_input_shape\u001b[0;34m(input_shape, default_size, min_size, data_format, require_flatten, weights)\u001b[0m\n\u001b[1;32m    399\u001b[0m                     )\n\u001b[1;32m    400\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0minput_shape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;36m3\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mweights\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"imagenet\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 401\u001b[0;31m                     raise ValueError(\n\u001b[0m\u001b[1;32m    402\u001b[0m                         \u001b[0;34m\"The input must have 3 channels; Received \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    403\u001b[0m                         \u001b[0;34mf\"`input_shape={input_shape}`\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: The input must have 3 channels; Received `input_shape=(32, 32, 1)`"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "early_stopping = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=3)\n",
        "model_checkpoint = tf.keras.callbacks.ModelCheckpoint('model.h5', monitor='val_accuracy', save_best_only=True)\n",
        "\n",
        "# Train the model with callbacks\n",
        "history = model.fit(train_data,\n",
        "                    epochs=10,\n",
        "                    validation_data=val_data,\n",
        "                    callbacks=[early_stopping, model_checkpoint])"
      ],
      "metadata": {
        "id": "KMHnbgzsp9bR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Evaluate the model on the test data\n",
        "test_loss, test_accuracy, test_recall, test_precision = resnet_model.evaluate(test_data)\n",
        "\n",
        "print('Test Loss:', test_loss)\n",
        "print('Test Accuracy:', test_accuracy)\n",
        "print('Test Recall:', test_recall)\n",
        "print('Test Precision:', test_precision)"
      ],
      "metadata": {
        "id": "S-GYylOxqAOF"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}